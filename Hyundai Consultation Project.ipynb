{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import of pyspark and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/home/dimitris13/spark-3.0.0-bin-hadoop3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We import the pyspark module\n",
    "\"\"\"\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We import a SparkSession\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We build a spark session, with name crew_pred, in order to \n",
    "access the Spark Web UI\n",
    "\"\"\"\n",
    "\n",
    "spark = SparkSession.builder.appName('cons_proj').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('/home/dimitris13/Downloads/cruise_ship_info.csv',\n",
    "                     header = True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explore some features of our data by using the appropriate Spark methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After we have imported our data, we rename some of the columns. In \n",
    "particular we capitalise some of the column titles. Of course, this\n",
    "is completely optional\n",
    "\"\"\"\n",
    "\n",
    "data = data.withColumnRenamed('passengers','Passengers').withColumnRenamed('length','Length').withColumnRenamed('cabins','Cabins')\n",
    "data = data.withColumnRenamed('passenger_density','Passenger_density').withColumnRenamed('crew','Crew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ship_name',\n",
       " 'Cruise_line',\n",
       " 'Age',\n",
       " 'Tonnage',\n",
       " 'Passengers',\n",
       " 'Length',\n",
       " 'Cabins',\n",
       " 'Passenger_density',\n",
       " 'Crew']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Ship_name='Journey', Cruise_line='Azamara', Age=6, Tonnage=30.276999999999997, Passengers=6.94, Length=5.94, Cabins=3.55, Passenger_density=42.64, Crew=3.55)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We obtain a list consisting of two objects. Those are row objects \n",
    "and correspond to the first two rows of the data frame. We pick\n",
    "the first row object of the list (i.e the first row of the Data \n",
    "Frame). Note that the actual crew members are data['crew']*100\n",
    "\"\"\"\n",
    "\n",
    "data.head(2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ship Journey has 355 crew members\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "An idea of exploring our data.\n",
    "\"\"\"\n",
    "\n",
    "data.head(2)[0][0]\n",
    "print(f'The ship {data.head(2)[0][0]} has {int(data.head(2)[0][-1]*100)} crew members')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now do some feature engineering on the columns of our Spark Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first observe that we have a categorical feature named 'Cruise_line'.\n",
    "The feature cannot be used while it is in this form. We shall \n",
    "convert it into numerical column by using the StringIndexer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We create a StringIndexer object, by giving the input column and\n",
    "defining the name of the output column. Then we fit the object in\n",
    "our data.\n",
    "\"\"\"\n",
    "\n",
    "string_indexer = StringIndexer(inputCol='Cruise_line',outputCol='Cruise_line_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = string_indexer.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_data = model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ship_name',\n",
       " 'Cruise_line',\n",
       " 'Age',\n",
       " 'Tonnage',\n",
       " 'Passengers',\n",
       " 'Length',\n",
       " 'Cabins',\n",
       " 'Passenger_density',\n",
       " 'Crew',\n",
       " 'Cruise_line_index']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We see that now, after the transformation, our Spark Data Frame\n",
    "has an extra column including the indices of the Cruise_line. Now we\n",
    "can utilise this column in our exploratory data analysis.\n",
    "\"\"\"\n",
    "indexed_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We now use the VectorAssembler object to gather all our feature \n",
    "columns in a single vector. This is the desired form of features\n",
    "in Apache Spark. The we transform our indexed_data.\n",
    "\"\"\"\n",
    "\n",
    "assembler = VectorAssembler(inputCols = ['Age', 'Tonnage', 'Passengers', 'Length', 'Cabins', 'Passenger_density', 'Cruise_line_index'],\n",
    "                           outputCol='features') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(indexed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All we need as our final data is the feature vector created by \n",
    "the VectorAssembler object and the labels which is the Crew column\n",
    "\"\"\"\n",
    "\n",
    "final_data = output.select(['features','Crew'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We perform a random split in our final_data, in order to split\n",
    "them in train and test set.\n",
    "\"\"\"\n",
    "\n",
    "train, test = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import and create an instance of a Linear Regression model. Note that we name our label column. Optionally we could have named our feature column, but the name 'feature' is the default name and there is no need to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(labelCol= 'Crew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is ready and fitted on our train data. We now need to evaluate our model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = regr_model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary = regr_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.728146442952454\n",
      "Mean Squared Error: 1.8840822298762954\n",
      "Root Mean Squared Error: 1.3726187489162078\n",
      "R2: 0.875531741555629\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean Absolute Error: {test_results.meanAbsoluteError}')\n",
    "print(f'Mean Squared Error: {test_results.meanSquaredError}')\n",
    "print(f'Root Mean Squared Error: {test_results.rootMeanSquaredError}')\n",
    "print(f'R2: {test_results.r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing the metrics above, we note that our model performs well and explains a big proportion of the variance of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall use our model to make predictions on unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data = test.select('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = regr_model.transform(unlabeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|            features|         prediction| Crew|\n",
      "+--------------------+-------------------+-----+\n",
      "|[5.0,160.0,36.34,...|  14.91298789368967| 13.6|\n",
      "|[6.0,30.276999999...|  4.131083118172464| 3.55|\n",
      "|[6.0,110.23899999...| 10.919425910041161| 11.5|\n",
      "|[6.0,113.0,37.82,...| 11.437956832857893| 12.0|\n",
      "|[8.0,110.0,29.74,...| 11.794433448744728| 11.6|\n",
      "|[9.0,81.0,21.44,9...|  9.299298972142367| 10.0|\n",
      "|[9.0,116.0,26.0,9...| 11.020301717326626| 11.0|\n",
      "|[10.0,86.0,21.14,...|  9.531191650580057|  9.2|\n",
      "|[11.0,58.6,15.66,...|  7.232518462769702|  7.6|\n",
      "|[11.0,86.0,21.24,...|   9.35111538306697|  9.3|\n",
      "|[11.0,90.09,25.01...|  8.817251861527899| 8.48|\n",
      "|[11.0,110.0,29.74...|  11.81533157475251| 19.1|\n",
      "|[12.0,2.329,0.94,...|0.24383414740290843|  0.6|\n",
      "|[12.0,50.0,7.0,7....|  4.578341343575277| 4.45|\n",
      "|[12.0,77.104,20.0...|  8.581093279824158| 9.59|\n",
      "|[12.0,88.5,21.24,...|    9.3484056015759|10.29|\n",
      "|[12.0,138.0,31.14...| 12.858149436990264|11.85|\n",
      "|[13.0,25.0,3.82,5...|  2.931909275896485| 2.95|\n",
      "|[13.0,91.0,20.32,...|  9.195439408747731| 9.99|\n",
      "|[13.0,101.509,27....| 10.866478333279964| 11.5|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.join(test,on = 'features').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some further exploratory data analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|Passengers and Crew correlation|\n",
      "+-------------------------------+\n",
      "|             0.9152341306065384|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(corr('Passengers','Crew').alias('Passengers and Crew correlation')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|Cabins and Crew correlation|\n",
      "+---------------------------+\n",
      "|         0.9508226063578497|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(corr('Cabins','Crew').alias('Cabins and Crew correlation')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is high positive (Pearson) correlation between the number of crew members and the numbers of passenger capacity and cabins in the ship. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
